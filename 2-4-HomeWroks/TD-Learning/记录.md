## TD算法中的$V_t$更新公式的例子
$V(S_{t}) \leftarrow V(S_{t})+\alpha\left[R_{t+1}+\gamma V(S_{t+1})-V(S_{t})\right]$
这个公式是**时间差分学习 (TD learning)** 的核心公式，具体是 **TD(0)** 的更新规则。它结合了蒙特卡洛方法（基于真实经验回报）和动态规划（基于估计的未来回报）的优点。让我详细解释一下这个公式的每一部分：
1. $V(S_{t})$: 这是我们当前对状态 $S_{t}$ 的预期回报的估计。
2. $\alpha$: 学习率，它决定了我们应该给新信息分配多少权重。
3. $R_{t+1}$: 这是在状态 $S_{t}$ 之后观察到的实际奖励。
4. $\gamma$: 折扣因子，它决定了未来奖励的重要性。如果 $\gamma$ 接近 1，我们就会高度重视未来奖励；如果 $\gamma$ 是 0，我们只关心即时奖励。
5. $V(S_{t+1})$: 这是我们当前对下一个状态 $S_{t+1}$ 的预期回报的估计。
整个方括号内的表达式 $[R_{t+1}+\gamma V(S_{t+1})-V(S_{t})]$ 是真实回报和我们的估计之间的误差，也被称为 **TD误差**。
这个更新规则的直观解释是：基于我们在 $S_{t}$ 观察到的奖励 $R_{t+1}$ 和我们对下一个状态 $S_{t+1}$ 的预期回报的估计，我们要调整我们对当前状态 $S_{t}$ 的预期回报的估计。
这种方法的优势在于它不需要等待整个轨迹或片段完成就可以更新值函数。每当我们观察到新的奖励和新的状态，我们都可以立即进行更新，这使得TD方法在实时学习环境中特别有用。

我们可以使用一个简单的迷宫游戏作为例子来说明这个TD(0)更新公式。
**环境：迷宫游戏**
你控制一个机器人，让它在迷宫中找到出口。迷宫如下：
```
+---+---+---+
| S |   | E |
+---+---+---+
```
- S: 起始位置
- E: 出口，到达这里会得到奖励 +1。
- 每个非终止步骤的奖励都是 0。
你的目标是从 S 到达 E。
**策略**：无论何时，都向右走。
**初始化**：
假设你对迷宫的了解很少，所以你初始化每个状态的值为0：
- $V(S) = 0$
- $V(\text{middle}) = 0$  （中间格子）
- $V(E) = 0$
**第一次尝试**：
你从 S 开始，向右走到中间格子，并收到奖励 $R_{t+1} = 0$。使用 TD(0) 更新公式：
$\alpha = 0.5$ ， $\gamma = 1$ (因为我们关心所有的未来回报)
更新 $V(S)$:
$$V(S) \leftarrow V(S) + \alpha \left[ R_{t+1} + \gamma V(\text{middle}) - V(S) \right]$$
$$V(S) \leftarrow 0 + 0.5 [0 + 1(0) - 0]$$
$$V(S) = 0$$
由于这一步没有任何新的信息，所以 $V(S)$ 保持不变。
然后你从中间格子走到 E，并得到奖励 $R_{t+1} = 1$。使用公式更新 $V(\text{middle})$:
$$V(\text{middle}) \leftarrow V(\text{middle}) + \alpha \left[ R_{t+1} + \gamma V(E) - V(\text{middle}) \right]$$
$$V(\text{middle}) \leftarrow 0 + 0.5 [1 + 1(0) - 0]$$
$$V(\text{middle}) = 0.5$$
这次你得到了新的信息，即从中间格子到达 E 会获得奖励，因此 $V(\text{middle})$ 的估计增加了。
下一次从状态S开始的时候，机器人选择了状态middle之后，也会更新V(S)
**结论**：
随着你玩这个游戏的次数越来越多，值函数会逐渐逼近真实值。TD(0) 更新公式使用每一步的奖励和下一状态的估计值来持续调整你对当前状态的预期回报的估计。

## MC算法中的$V_t$更新公式的例子
$V(s_t) \leftarrow V(s_t) + \alpha [ G_t - V(s_t) ]$
让我们使用一个简单的例子来说明这个更新公式。
假设你正在学习一个棋类游戏，其中每一局结束时，你可以获得 +1 的奖励（如果你赢了）或 -1 的奖励（如果你输了）。你想估计从某一特定局面 $s_t$ 开始到游戏结束的预期回报。
1. **初始化**:
   - 你初步估计该局面的值为 $V(s_t) = 0$。这是一个中性的估计，因为你还不知道这个局面是好还是坏。
   - 设定学习率为 $\alpha = 0.5$。
2. **第一局游戏**:
   - 从局面 $s_t$ 开始，你最终输了这局。
   - 真实回报 $G_t = -1$。
   - 根据更新公式: 
     $$\Delta V = \alpha [ G_t - V(s_t) ] = 0.5[-1 - 0] = -0.5$$
   - 更新估计值：$V(s_t) \leftarrow 0 - 0.5 = -0.5$。
3. **第二局游戏**:
   - 再次从局面 $s_t$ 开始，这次你赢了。
   - 真实回报 $G_t = +1$。
   - 根据更新公式: 
     $$\Delta V = \alpha [ G_t - V(s_t) ] = 0.5[1 + 0.5] = 0.75$$
   - 更新估计值：$V(s_t) \leftarrow -0.5 + 0.75 = 0.25$。
这个过程继续进行，每次你从局面 $s_t$ 开始的游戏，你都会更新你对该局面的预期回报的估计。
注意，这个例子展示了如何使用 Monte Carlo 方法，因为我们每次都是在游戏结束后进行更新。如果我们使用 TD 方法，我们会在每一步都进行估计，而不是等到游戏结束。