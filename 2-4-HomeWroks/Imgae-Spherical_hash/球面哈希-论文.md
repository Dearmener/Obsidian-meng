## 摘要
-----
近年来，许多基于散列的二进制编码方案得到了广泛的研究，因为它们能够提供高效的相似性搜索，特别是最近邻搜索，并且在许多计算机视觉问题中能够提供适合处理大规模图像数据库的紧凑数据表示。现有的散列技术通过使用基于超平面的散列函数来编码高维数据点。在本文中，我们提出了一种新的基于超空间的散列函数-球面散列，与基于超平面的散列函数相比，它可以将空间上更相关的数据点映射到二进制码中。此外，针对超空间二进制编码方案，提出了一种新的二进制编码距离函数--球面汉明距离，并设计了一种高效的迭代优化过程，以实现每个哈希函数数据点的均衡划分和哈希函数之间的独立性。我们的大量实验表明，我们的球形散列技术在大小从100万到7500万个GIST描述符的各种图像基准上的性能显著优于基于超平面的六种最先进的散列技术。性能收益是一致的，而且幅度很大，最高可提高100%。实验结果证实了该方法在高维空间中利用超球体对邻近区域进行编码的独特优点。最后，我们的方法是直观的，易于实现。

## 导论

由于数码相机和各种图像处理工具的快速发展，我们可以很容易地为各种目的创建新的图片和图像。这反过来又导致了大量的在线图片。这些巨大的图像数据库对许多计算机视觉应用程序的可扩展性提出了巨大的挑战，特别是那些需要高效相似性搜索的应用程序。
对于相似性搜索，最近邻搜索技术已经得到广泛研究，并且基于树的技术[6]已用于低维数据点。然而，这些技术在高维数据点上并不可扩展。因此，最近一直积极研究哈希技术，以提供高效的解决方案，适用于这种高维数据点[11, 23, 26]。
![[Pasted image 20231212154533.png]]
>这张图片似乎展示了一种数据散列技术，具体是利用超球形状来定义数据空间中的区域，并根据这些区域给数据点分配二进制编码。图中有三个子图，每个子图代表不同位数的编码方案：
>1. **左侧子图**：展示了一个2位（bit）的编码方案，其中有四个区域，每个区域对应一个不同的二进制代码：00、01、10、11。每个颜色代表一个超球，其交叉部分可能代表数据点可以属于多个区域。
>2. **中间子图**：表示一个3位编码方案，有八个不同的区域，每个区域由一个独特的3位二进制代码表示：000、001、010、011、100、101、110、111。这里超球的数量更多，表明可以划分出更细致的区域。
>3. **右侧子图**：看起来像是另一个2位编码方案的示例，但与左侧子图不同，它展示了一个更复杂的边界定义，可能是在更高维度空间中的投影，或者使用了不同的散列策略。这里的边界线条更为曲折，暗示了数据空间中更复杂的分隔。
整体而言，这张图片可能是在说明通过改变编码位数，如何使用超球来更精细地划分数据空间，并给出示例，显示不同编码位数和超球配置如何影响数据点的分配。这种技术可以用于高效的数据检索、分类或数据压缩。

将高维数据点根据哈希技术编码为二进制代码，通过这种方式实现更高的可扩展性，这要归功于其紧凑的数据表示和高效的索引机制。类似的高维数据点被映射到相似的二进制代码，因此通过仅查看这些相似的二进制代码（基于汉明距离），我们可以高效地识别近似最近邻。
现有的散列技术大致可分为与数据无关的方案和与数据有关的方案。在独立于数据的技术中，散列函数的选择与输入点无关。位置敏感散列（LSH）[11] 是这类技术中最广为人知的技术之一。该技术已扩展到各种散列函数 [3, 1, 13, 2, 20]。近期的研究重点已转向开发与数据相关的技术，以考虑数据点的分布并设计更好的散列函数。著名的例子包括频谱散列[26]、半监督散列[25]、迭代量化[7]、联合优化[10]和随机最大边距散列[15]。
在所有这些现有的散列技术中，超平面被用来将数据点（位于原始数据空间或内核空间中）划分为两组，并根据哪一组的数据点分配两个不同的二进制代码（如-1 或+1）每个点被分配到的集合。有别于这种传统方法，我们提出了一种新颖的基于超球的方案--球形散列，用于计算二进制代码。直观地说，超球在原始数据空间中定义更紧密的封闭区域的能力要比超平面强得多。例如，在一个 d 维空间中，至少需要 d + 1 个超平面才能定义一个封闭区域，而即使在一个任意高维空间中，也只有一个超球才能形成这样一个封闭区域。我们可以发现，核空间中的超平面能够映射到非线性散列函数（图 1）。然而，我们发现，在原始空间中提出的简单球形散列比最近的研究[10, 15, 20]中使用的非线性散列函数实现了更多的空间一致性分割。
我们的论文具有以下贡献：
1. 我们提出了一种新颖的球面哈希方案，分析了其在相似性搜索方面的能力，并将其与最先进的基于超平面的技术进行比较（第3.1节）。
2. 我们开发了一种专为球面哈希方法量身定制的新的二进制距离函数（第3.2节）。
3. 我们制定了一个优化问题，实现了每个哈希函数的平衡分区和任意两个哈希函数之间的独立性（第3.3节）。此外，提出了一种高效的迭代过程来构建球面哈希函数（第3.4节）。
为突显我们方法的优势，我们对我们的方法进行了测试，针对包含1到7500万个图像特征点的不同基准进行了测试，这些特征点具有不同的维度。我们还将我们的方法与许多最先进的技术进行了比较，发现我们的方法明显优于所有经过测试的技术，证实了与传统的基于超平面的哈希函数（第4节）相比，定义具有更紧密边界的封闭区域的卓越能力。

## 2. 相关工作
在这一部分，我们讨论与图像表示和最近邻搜索技术相关的先前工作。
### 2.1 图像表示
为了为查询图像识别视觉上相似的图像，已经研究了许多图像表示方法[4]。最受欢迎的方案示例包括视觉词袋表示法[21]和GIST描述符[19]，在实践中已被证明效果良好。这些图像描述符具有高维度（例如，数百到数千），通常将识别相似图像简化为在这些高维图像描述符空间中找到最近邻点[17]。由于这些图像描述符空间具有高维度，因此由于“维度灾难”[11]的原因，寻找最近邻图像描述符被认为非常困难。

### 2.2 树形方法
基于空间分割的树结构，如kd-trees [6]，已被用于寻找最近邻居并进行了更高性能的优化 [16]。然而，众所周知，对于高维点，基于kd-tree的搜索可能比线性扫描更慢。Nist´er 和 Stew´enius [18] 提出了另一种基于层次k均值树的最近邻居搜索方案。尽管这些技术在准确性和效率上取得了相当高的成就，但它们仅在包含约一百万张图像的小型图像数据库中进行了演示。此外，这些技术没有考虑压缩图像描述以处理大规模图像数据库

### 2.3 二进制哈希方法
二进制哈希方法旨在将数据点嵌入二进制代码，同时保持它们之间的相对距离。其中一种最流行的哈希技术是LSH [11]。其哈希函数基于投影到从特定分布中绘制的随机向量。已经为学习度量提出了LSH的许多变体，如min-hash [2]，随机傅立叶特征 [20]等 [3, 1]。由于LSH中的哈希函数是独立于数据点绘制的，对于短长度的二进制代码，这可能是低效的。
有许多研究致力于开发数据相关的哈希方法，这些方法反映了数据分布，以提高性能。Weiss等人[26]提出了一种基于数据的、受谱图分割启发的谱哈希方法。它在特别是对于紧凑的比特长度（即8位和16位）的情况下改善了LSH的性能。Wang等人[25]提出了一种半监督哈希方法，通过利用训练集的标签信息来提高图像检索性能。Gong和Lazebnik [7]引入了一种普鲁克斯特方法，通过旋转零中心的PCA投影数据直接最小化量化误差。He等人[10]提出了一种同时优化搜索精度和搜索时间的哈希方法。Joly和Buisson [15]通过使用大边界分类器构建哈希函数，该函数使用任意抽样的数据点，这些数据点随机分为两组。
所有提到的哈希技术通过在超平面上将数据点（位于原始特征空间或核空间中）分为两个不同的集合来计算二进制编码。离开这种传统方法，我们采用了一种新颖的方法，通过超球体对数据点进行分区。
我们发现了一个同名方法，球形LSH [22]。我们的方法与这个球形LSH完全不同，这是一种专门用于位于单位超球面上的数据点的技术。
### 2.4 基于距离的索引方法
数据库社区一直在设计高维点索引和支持各种邻近查询的高效技术。Filho等人[5]使用距离固定枢轴点的方法索引点。结果，给定一个枢轴的区域变成了一个环形。通过使用多个枢轴进一步减小了该区域。然后，他们构建了各种层次结构（例如R树）来支持各种邻近查询。这种方法的效率高度依赖于枢轴的位置。Jagadish等人[12]在选择枢轴时使用了k均值聚类，Venkateswaran等人[24]采用了其他启发式方法，如最大化从枢轴到数据点的距离的方差。
这项工作在使用从枢轴到高维点的距离进行索引方面与我们的概念相似。然而，我们的方法与这些技术截然不同，因为我们的目标是通过使用哈希来计算保留特征点原始度量空间的紧凑二进制代码，而他们的目标是设计支持高效近邻查询的分层索引结构。

## 3 球面哈希
让我们首先定义记号。给定一个$n$个数据点的集合在一个$D$-维空间，我们使用 $X = \{x_1, ..., x_n\}, x_i \in \mathbb{R}^D$来表示这些数据点。对应于每个数据点$x_i$的二进制码由$b_i = \{-1, +1\}^c$定义，其中$c$是码的长度。
### 3.1 二进制编码嵌入功能
我们的哈希函数$H(x) = (h_1(x), ..., h_c(x))$将$\mathbb{R}^D$中的点映射到二进制立方体$\{-1, +1\}^c$中。我们使用一个超球面来定义一个球形哈希函数。每个球形哈希函数$h_k(x)$由一个枢轴$p_k \in \mathbb{R}^D$和一个距离阈值$t_k \in \mathbb{R}^+$定义如下：
$$
h_k(x) =
  \begin{cases}
    -1 & \text{当 } d(p_k, x) > t_k \\
    +1 & \text{当 } d(p_k, x) \leq t_k
  \end{cases}
$$
其中$d(\cdot, \cdot)$表示两个点在$\mathbb{R}^D$中的欧几里得距离，可以使用不同的距离度量（例如，$L_p$度量）来代替欧几里得距离。每个球形哈希函数$h_k(x)$的值表明点$x$是否在中心为$p_k$且半径为$t_k$的超球面内部。
使用超平面和超球面计算二进制码的关键区别在于它们定义$\mathbb{R}^D$中可以由二进制码索引的封闭区域的能力。为了在$d$-维空间定义一个封闭区域，至少需要$d + 1$个超平面，而只需一个超球面就足以形成这样一个封闭区域，在任意高维空间中也是如此。此外，使用多个超平面可以构造更多的封闭区域，而使用多个超球面，尽管位于每个区域内的点之间的距离是有界的，也可以构造出无界的区域。例如，通过使用$c$个超球面，有界区域的数量可以增加到$(c-1)$加上$d$维中所有$i$从$0$到$c$的$C(i)$的总和[27]。另外，我们可以用一个大半径和一个远离中心的超球面来近似一个超平面。
在最近邻搜索中，形成具有更紧密距离界限的封闭区域的能力在有效定位查询点附近的最近邻方面非常重要。当我们构建这样更紧密的封闭区域时，由查询点的二进制代码索引的区域可以包含更多有望成为最近邻候选的点。
我们还从经验上测量了超球体和超平面包围区域的紧密程度。为此，我们测量具有相同二进制代码的任意两点之间的最大距离，并取不同二进制代码之间最大距离的平均值。正如在图2的左侧图中所示，与LSH中使用的超平面相比，超球体更紧密地包围了二进制代码的区域[3]。在所有测试的代码长度上，超球体的边界约为超平面方法的两倍。

### 3.2 二进制代码之间的距离
大多数基于超平面的二进制嵌入方法使用汉明距离来衡量两个二进制码之间的差异，即不同位的数量，也就是说，$|b_i \oplus b_j|$，其中$\oplus$是XOR位运算，而$| \cdot |$表示在给定二进制码中+1位的数量。这种距离度量衡量了两个给定点分别位于多少个超平面的对立面。然而，汉明距离并不很好地反映与定义具有更紧密边界的封闭区域相关的属性，而这正是我们使用球形哈希函数的核心优势。
为了充分利用我们球形哈希函数的理想属性，我们提出了以下距离度量，即球形汉明距离（$d_{shd}(b_i, b_j)$），用于计算通过球形哈希得到的两个二进制码$b_i$和$b_j$之间的距离：
$$
d_{shd}(b_i, b_j) = \frac{|b_i \oplus b_j|}{|b_i \land b_j|}
$$
其中，$|b_i \land b_j|$表示两个二进制码之间共同+1位的数量。
在两个二进制码中具有共同的+1位能够使我们在球形哈希函数中获得更紧的边界。这主要是因为每个共同的+1位表明两个数据点位于其对应的超球面内部，这为这两个数据点的距离边界提供了更强的线索。为了观察距离边界和共同+1位数量之间的关系，我们测量了数据点的平均距离边界作为共同+1位数量的函数。如图2所示，两个二进制码中共同+1位数量增加，平均距离边界减小。因此，我们在我们球形汉明距离的分母中放入了 $|b_i \land b_j|$。在实现中，我们向分母添加了一个小值以避免除以零的情况。
两个二进制码之间的共同+1位定义了一个如上所述具有距离边界的封闭区域。在这个封闭区域内，我们可以基于汉明距离 $|b_i \oplus b_j|$，即我们距离函数的分子，进一步区分两个二进制码之间的距离。分子以与汉明距离相同的方式影响我们的距离函数，因为当两个二进制码之间有更多不同的位时，它们之间的距离就会增加。

### 3.3 哈希函数之间的独立性
函数实现每个哈希函数的数据点均衡分区和哈希函数之间的独立性被认为是重要的[26, 10, 15]，因为独立的哈希函数以平衡的方式分布点到不同的二进制代码中。已知实现这样的属性可以减少搜索时间[10]，甚至对于更长的位长度也能提高准确性[15]。我们也旨在实现我们的球形哈希函数之间的独立性。
我们定义每个哈希函数 $h_k$ 对于 +1 和 -1 位分别具有相等的概率如下：
$$
Pr[h_k(x) = +1] = \frac{1}{2} ; \quad x \in X, \quad 1 \leq k \leq c \quad (1)
$$
让我们定义一个概率事件 $V_k$ 来表示 $h_k(x) = +1$ 的情况。两个事件 $V_i$ 和 $V_j$ 是独立的当且仅当 $Pr[V_i \cap V_j] = Pr[V_i] \cdot Pr[V_j]$。一旦我们实现了每个位的数据点的平衡划分（方程1），那么两个位之间的独立性可以用下面的方程满足，给定 $x \in X$ 和 $1 \leq i, j \leq c$：
$$
Pr[h_i(x) = +1, h_j(x) = +1] = Pr[h_i(x) = +1] \cdot Pr[h_j(x) = +1] = \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4} \quad (2)
$$
一般来说，哈希函数之间的两两独立性并不能保证三个或更多哈希函数之间的高阶独立性。我们还可以表述超过两个哈希函数之间的独立性，并在满足等式1和等式2中显示的约束的同时努力满足它们。然而，我们发现考虑这种高阶独立性几乎不会改善搜索质量。

### 3.4 迭代优化
我们现在提出一个迭代过程来计算$c$个不同的超球面，即它们的枢轴$p_k$和距离阈值$t_k$。在这个迭代过程中，我们构建超球面以满足方程1和方程2中显示的约束。
作为我们迭代过程的第一阶段，我们从数据点$X$中抽样一个子集$S = \{s_1, s_2, ..., s_m\}$来近似其分布。然后我们用子集$S$中随机选取的$c$个数据点初始化$c$个超球面的枢轴；我们发现使用其他初始化枢轴的方法（例如，对子集$S$执行K-均值聚类得到的中心点）并不影响我们优化过程的结果。
作为我们迭代过程的第二阶段，我们细化超球面的枢轴并计算它们的距离阈值。为了帮助这些计算，我们计算以下两个变量，$o_i$和$o_{i,j}$，给定$1 \leq i, j \leq c$：
$$
o_i = |\{s_k|h_i(s_k) = +1, 1 \leq k \leq m\}|,
$$
$$
o_{i,j} = |\{s_k|h_i(s_k) = +1, h_j(s_k) = +1, 1 \leq k \leq m\}|,
$$
其中 $| \cdot |$ 是给定集合的基数。$o_i$衡量子集$S$中有多少数据点在第$i$个哈希函数中有+1位，将用于满足每个位的平衡分割（方程1）。同时，$o_{i,j}$衡量在子集$S$中包含在对应于第$i$和第$j$个哈希函数的两个超球面内的数据点数量，将用于在我们的迭代优化过程中满足第$i$和第$j$个哈希函数之间的独立性。
一旦我们用子集$S$中的数据点计算出这两个变量，我们采用两个交替步骤来重新定义超球面的枢轴和距离阈值。首先，我们调整两个超球面的枢轴位置，使得$o_{i,j}$更接近或等于$\frac{m}{4}$。直观地说，对于每对超球面$i$和$j$，当$o_{i,j}$大于$\frac{m}{4}$时，一个排斥力作用在这两个枢轴上（即$p_i$和$p_j$），使它们彼此远离。否则，一个吸引力作用使它们靠近。其次，一旦枢轴计算出来，我们调整第$i$个超球面的距离阈值$t_i$，使得$o_i$变成$\frac{m}{2}$以满足数据点的平衡分割（方程1）。
我们执行迭代过程直到计算出的超球面不再在满足约束方面有进一步的改进。具体来说，我们考虑$o_{i,j}$的均值和标准差作为我们迭代过程收敛的度量。理想的均值和$o_{i,j}$的标准差分别是$\frac{m}{4}$和零。然而，为了避免过拟合，当$o_{i,j}$的均值和标准差在理想均值的$\varepsilon_m\%$和$\varepsilon_s\%$的误差容忍度范围内时，我们停止迭代过程；我们发现太低的误差容忍度会导致过拟合，而接近10%的值给出合理的结果。在接下来的实验中，我们始终使用$\varepsilon_m$和$\varepsilon_s$的固定值分别为10%和15%。
**力的计算**：从$p_j$到$p_i$的（排斥或吸引）力，$f_{i \rightarrow j}$，被定义如下（见图3）：
$$
f_{i \rightarrow j} = \frac{1}{2} \left( \frac{o_{i,j}}{m/4} - 1 \right) (p_i - p_j).
$$
然后累积的力$f_i$是所有其他枢轴计算出的力的平均值如下：
$$
f_i = \frac{1}{c} \sum_{j=1}^{c} f_{i \rightarrow j}.
$$
一旦我们将累积的力$f_i$应用到$p_i$上，那么$p_i$就简单地更新为$p_i + f_i$。我们的迭代优化过程如算法1所示。
我们迭代过程的时间复杂度是$O((c^2 + cD)m)$，这与当前最先进技术（例如，谱哈希的$O(D^2m)$）相当。实际上，我们的迭代过程在10到30次迭代内完成。此外，它的整体计算时间即使对于128位码长度也不到30秒。收敛率与迭代次数的关系如图4所示。请注意，我们的迭代优化过程与N体模拟的特性相似，N体模拟是为模拟各种动态系统的粒子（例如，天体相互作用的对象）在引力作用下的相互作用而设计的。有效的数值积分方法（例如，快速多极方法）可以应用于加速我们的迭代优化过程。

## 4 评估
### 4.1 数据集和协议
我们使用以下三个数据集进行了各种实验：
- **GIST-1M-384D**：一组384维，包含100万个GIST描述符的集合，它包含了80万Tiny Images数据集的一个子集【23】。
- **GIST-1M-960D**：一组960维，包含100万个GIST描述符的集合，这些也在【14】中使用。
- **GIST-75M-384D**：一组384维，包含7500万个GIST描述符的集合，它包含了80万Tiny Images数据集的一个子集【23】。
我们的评估协议遵循【15】。具体来说，我们为**GIST-1M-384D**和**GIST-1M-960D**数据集随机选择了1000个查询，以及500个与数据点没有任何重叠的**GIST-75M-384D**查询。性能通过平均精确度均值（mAP）来衡量。基准真相是由基于欧几里得距离的穷尽式线性扫描计算的$k$个最近邻定义的。在计算精确度时，我们考虑所有具有较低或相等汉明距离（或球形汉明距离）的项。
对于**GIST-1M-384D**和**GIST-1M-960D**的实验，我们使用一个包含i7 X990和24GB主内存的机器。对于**GIST-75M-384D**，我们使用一个包含Xeon X5690和144GB主内存的机器来容纳所有数据在其主内存中。

### 4.2 对比方法
- **LSH和LSH-ZC**：局部敏感哈希（LSH）[3]与/不带零中心数据点。投影矩阵是一个高斯随机矩阵。正如在[8, 7]中讨论的那样，数据中心化（即，$\sum x_i = 0$）产生了比LSH更好的结果。因此，我们转换数据点，使它们的中心位于LSH-ZC的原点。
- **LSBC**：局部敏感二进制码（LSBC）[20]。实验中使用的带宽参数是数据集中点的平均距离的倒数，如[8]中建议的那样。
- **SpecH**：谱哈希[26]。
- **PCA-ITQ**：迭代量化[7]。
- **RMMH-L2**：随机最大边缘哈希（RMMH）[15]与三角形L2核。我们实验了带有三角形L2核的RMMH，因为作者报告了使用此核的k近邻搜索的最佳性能。我们使用参数M，即每个哈希函数的样本数量为32，如[15]所建议。
- **GSPICA-RBF**：广义相似性保持独立成分分析（GSPICA）[10]与RBF核。我们使用RBF核实验GSPICA，因为作者报告了使用此核的k近邻搜索的最佳性能。RBF核中使用的参数由训练样本中第k近邻的平均距离确定，如[15]所建议。参数$\gamma$和$P$分别为1和数据集维度，如[10]中建议。
- **我们的-HD和我们的-SHD**：我们测试了我们方法的两个不同版本。我们的-HD代表我们的方法与常见的汉明距离，而我们的-SHD使用我们的球形汉明距离。
对于所有的数据依赖哈希方法，我们随机选择了原始数据集中的10万个数据点作为训练集。我们还使用相同的训练集来估计每种方法的参数。我们通过重复所有实验五次来报告平均精确度均值（mAP）和召回值，以获得统计意义上的值；对于**GIST-75M-384D**基准，我们只重复了三次实验，因为它的实验时间较长。请注意，我们没有报告两种基于PCA的方法**SpecH**和**PCA-ITQ**在384维数据集上512个哈希位的结果，因为它们不支持比数据空间维度更大的位长度。