---
creationDate: 2023-10-17 17:17
card_type: 机器学习
---
课程中的例子过于抽象，这里举一个具体的实例
当我们说使用整个训练数据集计算损失函数的梯度时，实际上是指在每次迭代中，我们都会对整个数据集中的所有样本进行前向传播和反向传播，从而计算整体的平均梯度。

以下是一个简化的例子来解释这个过程。假设我们有一个线性回归任务，目标是找到最佳的参数 $w$ 和 $b$ 来最小化均方误差。给定训练数据集 $X$ 和相应的目标值 $y$。
这个线性回归任务的y为:$wx_i+b$
## **损失函数**:
使用均方误差损失函数
$L(w, b) = \frac{1}{m} \sum_{i=1}^{m} (w x_i + b - y_i)^2$
其中，$m$ 是样本的数量。
## **梯度计算**:
对于 $w$ 和 $b$，我们可以分别计算损失函数的偏导数（梯度）。

$$ \frac{\partial L}{\partial w} = \frac{2}{m} \sum_{i=1}^{m} x_i(w x_i + b - y_i)$$

 $$\frac{\partial L}{\partial b} = \frac{2}{m} \sum_{i=1}^{m} (w x_i + b - y_i) $$

在每次迭代中，使用上面的公式，我们可以计算整个训练数据集的梯度。然后，我们使用这个梯度来更新 $w$ 和 $b$。

## **参数更新**:
$$ w := w - \alpha \frac{\partial L}{\partial w} =\frac{2}{m} \sum_{i=1}^{m}x_i(w x_i + b - y_i)$$
**这里的$\sum\limits$ 就体现出了批量的概念**
$$ b := b - \alpha \frac{\partial L}{\partial b} = \frac{2}{m} \sum_{i=1}^{m} (w x_i + b - y_i) $$

其中，$\alpha$ 是学习率，一个超参数。

这个过程就是使用整个数据集计算损失函数的梯度。注意，这只是一个简化的线性回归示例，真实的深度学习模型可能涉及更多的参数和复杂的损失函数，但基本的原理是相似的：使用所有的数据计算平均梯度，然后更新参数。