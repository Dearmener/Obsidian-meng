## 背景介绍
![[Pasted image 20231204163427.png]]

马尔科夫性：$P(x_{t+1} | x_t,  x_{t-1}, ..., x_0) = P(X_{t+1} | x_t)$ ，前状态的未来发展只依赖于当前状态，而不依赖于过去状态。
马尔科夫链：具有马尔科夫性的随机过程
状态空间：马尔科夫链+观测
![[Pasted image 20231204164852.png|400]]
马尔科夫奖励过程（MRP）：马尔科夫链+奖励
马尔科夫决策过程（MDP）：马尔科夫链+奖励+动作
$$
\begin{align*}
\mathbf{S}: & \text{state set} \rightarrow S_t \\
\mathbf{A}: & \text{action set} \rightarrow A_t \\
\mathbf{R}: & \text{reward set} \rightarrow R_t \text{ 或者 } R_{t+1}
\end{align*}
$$

## 价值函数
在马尔可夫决策过程（MDP）中，价值函数是一个关键概念，它用于评估在特定状态下采取特定策略的期望回报。MDP 中有两种主要的价值函数：状态价值函数和动作价值函数。
1. **状态价值函数 $V_{\pi}(s)$**: 这个函数评估在状态 $s$ 下，遵循特定策略时，一个代理（agent）可以获得的期望回报。它是未来所有可能回报的期望值，通常用贝尔曼方程来表示：
 $$ V(s) = \sum_{a \in A} \pi(a|s) \sum_{s', r} P(s', r | s, a) [r + \gamma V(s')]$$
 其中，$\pi(a|s)$ 是在状态 $s$ 下选择动作 $a$ 的策略，$P(s', r | s, a)$ 是从状态 $s$ 通过动作 $a$ 转移到状态 $s'$ 并获得回报 $r$ 的概率，$\gamma$ 是折扣因子，表示未来回报的当前价值。
2. **动作价值函数 $Q(s, a)$**: 这个函数评估在状态 $s$ 下采取动作 $a$ 并遵循特定策略的期望回报。它可以用类似的贝尔曼方程表示：
 $$ Q(s, a) = \sum_{s', r} P(s', r | s, a) [r + \gamma \sum_{a' \in A} \pi(a'|s') Q(s', a')]$$
 这里，$Q(s', a')$ 表示在下一个状态 $s'$ 下采取动作 $a'$ 的价值。
在 MDP 中，目标通常是找到最优策略，即最大化状态价值函数 $V(s)$ 或动作价值函数 $Q(s, a)$ 的策略。这可以通过各种强化学习方法实现，如值迭代、策略迭代或Q学习。

状态价值函数 $V(s)$ 和动作价值函数 $Q(s, a)$ 在马尔可夫决策过程（MDP）中密切相关，它们之间的关系可以用以下方式描述：
1. **从 $Q(s, a)$ 到 $V(s)$**: 状态价值函数 $V(s)$ 可以从动作价值函数 $Q(s, a)$ 通过考虑特定策略下所有可能的动作和这些动作的概率分布得到。具体来说，$V(s)$ 是在状态 $s$ 下，遵循策略 $\pi$ 时所有可能动作的 $Q(s, a)$ 的期望值。数学上表示为：
 $$ V(s) = \sum_{a \in A} \pi(a|s) Q(s, a)$$
 这里，$\pi(a|s)$ 是在状态 $s$ 下根据策略 $\pi$ 选择动作 $a$ 的概率。
2. **从 $V(s)$ 到 $Q(s, a)$**: 相反地，动作价值函数 $Q(s, a)$ 可以通过考虑在状态 $s$ 下采取动作 $a$ 并随后遵循特定策略的期望回报来从状态价值函数 $V(s)$ 得到。这表示为：
 $$ Q(s, a) = \sum_{s', r} P(s', r | s, a) [r + \gamma V(s')]$$
 这里，$P(s', r | s, a)$ 是从状态 $s$ 通过动作 $a$ 转移到状态 $s'$ 并获得回报 $r$ 的概率，$\gamma$ 是折扣因子。
简而言之，$V(s)$ 提供了在给定状态下遵循特定策略的总体期望回报，而 $Q(s, a)$ 提供了在给定状态下采取特定动作并随后遵循特定策略的期望回报。在实际应用中，这两个函数常常相互依赖，用于迭代地改进策略以求达到最优解。

