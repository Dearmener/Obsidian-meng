---
creationDate: 2023-10-19 09:11
card_type: 人工智能
---
马尔科夫奖励过程（Markov Reward Process，简称MRP）是在马尔科夫过程的基础上增加了奖励 R 和衰减系数 γ 的概念。具体来说，MRP是一个由 <S,P,R,γ> 组成的元组：

- **S** 是有限的状态集。
- **P** 是状态转移概率矩阵。对于任意两个状态 s 和 s'，$P_{SS'} = P [S_{t+1}=s'|S_{t}=s]$ ，表示在时刻 t 处于状态 s 的条件下，在时刻 t+1 转移到状态 s' 的概率。
- **R** 是一个奖励函数。对于任意状态 s，$R_{s} = E [R_{t+1} | S_{t} = s ]$ ，表示在时刻 t 处于状态 s 的条件下，在时刻 t+1 能获得的奖励期望。
- **γ** 是衰减系数，取值范围为 $[0,1]$。

在强化学习中，我们给这个累积奖励一个新的名称：收获（Return，或回报）。收获 G_{t} 为在一个马尔科夫奖励链上从 t 时刻开始往后所有的奖励的有衰减的收益总和。具体来说，如果我们定义 R_{t} 为 t 时刻获得的奖励，那么收获 G_{t} 可以表示为：

$$G_{t} = R_{t+1} + γR_{t+2} + γ^{2}R_{t+3} + ... = \sum_{k=0}^{∞} γ^{k}R_{t+k+1}$$

这里的 γ 是衰减因子，体现了未来的奖励在当前时刻的价值比例。如果 γ 接近0，则表明趋向于“近视”性评估；如果 γ 接近1则表明偏重考虑远期的利益。

此外，马尔科夫奖励过程中还定义了价值函数（Value function），它给出了某一状态或某一行为的长期价值。对于任意状态 s，其价值函数 v(s) 定义为从该状态开始的马尔科夫链收获的期望：

$$v(s) = E [ G_{t} | S_{t} = s ]$$

这里的期望是因为从 t 时刻到终止状态的马尔科夫链不止一条，每一条都有对应的概率和 Return 收益。

防守打法