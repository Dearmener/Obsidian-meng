AdaBoost推导
AdaBoost，全称为Adaptive Boosting，是一种集成学习算法，它通过结合多个弱学习器来构建一个强学习器。在每一轮中，它调整样本的权重，对前一轮分类错误的样本给予更多关注，然后增加一个新的弱学习器来补偿之前的分类错误。以下是AdaBoost的基本推导步骤：
### 初始化权重
给定一个训练集 $\{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\}$，其中每个 $x_i$ 是一个特征向量，$y_i$ 是对应的标签，通常 $y_i \in \{-1, +1\}$。初始化每个样本的权重 $D_1(i) = \frac{1}{N}$，对所有 $i = 1,2,...,N$。
### 迭代训练弱学习器
对于每一轮 $t = 1, 2, ..., T$：
1. 训练一个弱学习器 $h_t$ 使用加权样本。弱学习器的目标是最小化加权错误率：
 $$
 \epsilon_t = \frac{\sum_{i=1}^N D_t(i) \cdot \mathbb{I}(h_t(x_i) \neq y_i)}{\sum_{i=1}^N D_t(i)}
 $$
 其中 $\mathbb{I}$ 是指示函数，当 $h_t(x_i) \neq y_i$ 时取值为1，否则为0。
2. 计算弱学习器的权重：
 $$
 \alpha_t = \frac{1}{2} \ln\left(\frac{1 - \epsilon_t}{\epsilon_t}\right)
 $$
 这个权重反映了弱学习器的性能，误差越小，权重越大。
3. 更新样本权重：
 $$
 D_{t+1}(i) = \frac{D_t(i) \cdot \exp(-\alpha_t y_i h_t(x_i))}{Z_t}
 $$
 其中 $Z_t$ 是一个归一化因子，确保 $D_{t+1}$ 是一个概率分布。
### 得到最终模型
经过T轮迭代后，得到T个弱学习器及其权重，最终模型是这些弱学习器的加权组合：
$$
H(x) = \text{sign}\left(\sum_{t=1}^T \alpha_t h_t(x)\right)
$$
最终的分类器 $H(x)$ 表示的是，对于每个样本，通过所有弱学习器的加权多数投票来决定最终的预测标签。这里的“sign”函数用于得到最终预测的类别，如果结果是正数则为+1，如果是负数则为-1。
AdaBoost的关键特性之一是它能够在每一轮中调整样本的权重，重点关注那些之前被错误分类的样本。通过这种方式，弱学习器能够在后续的训练中纠正先前的错误。
