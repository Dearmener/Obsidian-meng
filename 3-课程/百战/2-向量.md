
向量是一个一维数组，向量的几何意义就是空间中的点，物理意义是速度或者力这样的矢量。向量的**分量**为维度，n维向量集合的全体就构成了n维欧式空间 $R^n$。可以通过升维度对样本进行更好的区分。在机器学习中，每一个元素就是一个特征。

# 行向量
给出一个行向量
> [!ai]+ AI
>
> 当您说到一个行向量时，您是否有特定的向量值或元素要求？如果有，请提供向量的具体数值或元素，这样我可以更好地帮助您。如果您只是想了解如何表示一个行向量，通常可以使用方括号或圆括号来表示，例如：
> $$\mathbf{v} = [1, 2, 3] $$
> 或者
> $$\mathbf{v} = (1, 2, 3) $$
> 这是一个包含三个元素的行向量示例。如果您需要一个不同大小或不同元素的向量，请
> 提供更多详细信息。

转置 Transpot
在`numpy`中可以使用`reshape(-1,1)`对行向量进行列转置

### 向量的运算
**内积** 
两个列向量，$X^TY$，等于对应位置相乘再相加
两个向量的内积本质就是变成一个标量
在Python使用a.dot(b)
> [!ai]+ AI
>
> 是的，两个向量的内积（也称为点积）将两个向量映射成一个标量值。内积的计算方法是将两个向量对应位置的元素相乘，然后将这些乘积相加起来，最终得到一个标量结果。内积在线性代数和向量空间中有广泛的应用，可以用于测量向量之间的相似性、计算投影等等。它是向量运算中的重要概念之一。

给出一个内积的例子
> [!ai]+ AI
>
> 内积（也称点积或数量积）是向量的数学运算，通常表示为两个向量的乘积再求和。这里给你一个简单的内积示例：
> 假设有两个三维向量：
> 向量A = [2, 3, 4]
> 向量B = [1, -1, 2]
> 它们的内积可以通过以下方式计算：
> 内积$(A, B) = A · B = (2 * 1) + (3 * (-1)) + (4 * 2) = 2 - 3 + 8 = 7$
> 所以，向量A和向量B的内积等于7。

**数乘**
一个数和这个向量每个分量相乘，梯度下降法会用到

**转置**
把列变成行，把行变成列。转置的意义：便于内积5t555555555rt55r55555555r
> [!ai]+ AI
>
> 矩阵的转置是一种操作，它将矩阵的列变成行，行变成列。这种操作通常在数学和计算机科学中用于多种应用，其中一些意义包括：
> 1. 数据分析：在数据科学和统计学中，矩阵的转置可以用于重新组织数据以进行分析。这对于数据清理和建模非常有用。
> 2. 矩阵运算：在线性代数和数值计算中，矩阵的转置是一种常见的操作，用于解线性方程组、计算特征值和特征向量等。
> 3. 图像处理：在图像处理中，图像通常以矩阵的形式表示。转置可以用于旋转图像或进行其他变换。
> 4. 神经网络：在深度学习中，神经网络的权重矩阵通常会进行转置以适应网络的结构。
> 总之，矩阵的转置在多个领域中都有重要的应用，它允许数据和矩阵在不同的上下文中更灵活地使用。


## 向量的范数
范数的通用公式：通用的 $L^p$ 范数定义为： $$ ||x||_p = \left( \sum_i^n |x_i|^p \right)^{\frac{1}{p}} $$ 其中 $p\geq1$。
> [!ai]+ AI
>
> 范数（Norm）通常用于衡量向量的大小或长度。在线性代数中，常见的范数包括L1范数、L2范数等，它们的公式如下：
> 1. L1范数（也称为曼哈顿范数）：
> ||x||₁ = |x₁| + |x₂| + ⋯ + |xₙ|
> 2. L2范数（也称为欧几里德范数）：
> ||x||₂ = √(x₁² + x₂² + ⋯ + xₙ²)
> 这些范数用于不同的应用中，L1范数通常用于稀疏性相关的任务，而L2范数常用于回归和优化问题。除此之外，还存在其他范数，如L∞范数等，其公式类似，但具体定义略有不同。

它为向量空间中的每个向量赋予一个非负的标量值，通常表示为“长度”或“大小”。在机器学习中范数对应着正则项，一个向量的$L_{1},L_{2}$正则项越小，说明机器的泛化能力越强，越不可能过拟合。

在Python中求解范式：
```python
# 求解L1范式
w = np.array([3,5,6,7,2,-1])
L1 = np.abs(w) # 先取绝对值
np.sum(L1) # 再求和

# 求解L2范式
L2 = np.square(w) # 先给每个元素取平方
L2_sum = np.sum(L2) #再将每个元素相加
np.sqrt(L2_sum)#再整体开个平方
```
## 特殊的向量
0向量：就是分量全为0的向量,$\mathbf{0} = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}$，意义何在：机器学习中的bias进行初始化的时候，会将其初始化为0向量。

全为1的向量：分量全为1的向量，用在线性回归，逻辑回归中使用。

稀疏向量：0很多

稠密向量：0较少

单位向量：就是L2范数为1的向量。如何获得一个单位向量：使用向量除以该向量的模（L2范式）。
$$\mathbf{u} = \frac{\mathbf{w}}{||\mathbf{w}||_2}$$
如果将一个向量单位化后获得一个单位向量，该单位向量与原向量方向保持一致，单位向量代表着方向
