---
标题: "Chapter 6:Temporal-Diﬀerence Learning · Reinforcement Learning:An Introduction(2nd)"
url: https://laddie132.github.io/Reinforcement-Learning-Notes/chapter/chapter6.html
创建时间: 2023-10-29 23:43
更新时间: 2023-10-29 23:43
笔记ID: H8H5839
收藏: 0
划线数量: 1
标签: 
obsidianUIMode: preview
---

# Chapter 6:Temporal-Diﬀerence Learning · Reinforcement Learning:An Introduction(2nd) 

#Wucai

> [阅读原文](https://laddie132.github.io/Reinforcement-Learning-Notes/chapter/chapter6.html)
> [在五彩中查看](https://marker.dotalk.cn/#/?noteidx=H8H5839)


## 划线列表
%%begin highlights%%
> [!Annotation]
> <font color="#FFFF83">◇  </font> 对于批处理更新，首先固定值函数，将一个片段中每一个时间点的值函数累计更新相加，在片段结束时，统一对值函数进行一次更新

> 批处理更新是强化学习中的一种策略，尤其用于有限的数据样本或资源有限的环境中。相比于在线或每步更新，批处理更新的主要思想是累积一系列的更新，然后在一定的时间点进行一次统一的更新。

考虑一个经验轨迹或片段，其由一系列的状态、动作、奖励组成：\( (s_1, a_1, r_1), (s_2, a_2, r_2), \ldots, (s_T, a_T, r_T) \)，其中 \( T \) 为片段的终点。

对于时差学习（TD 学习）来说，每一步的 TD 错误可以表示为：

\[ \delta_t = r_{t+1} + \gamma v(s_{t+1}) - v(s_t) \]

在批处理更新中，我们不会立即使用这个 TD 错误来更新值函数，而是将整个片段中的所有 TD 错误累积起来。具体地，我们可以计算整个片段的累积 TD 错误：

\[ \Delta V = \sum_{t=1}^{T} \alpha_t \delta_t \]

其中 \( \alpha_t \) 是在时间 \( t \) 的学习率。

片段结束时，我们用累积的 TD 错误来进行一次统一的值函数更新：

\[ v(s_t) = v(s_t) + \Delta V \]

这样，我们只进行了一次实际的值函数更新，而不是在片段中的每一步都进行更新。这种方法有其优势，例如可以减少值函数的震荡，提高算法的稳定性，并在计算资源有限的情况下节省计算成本。

然而，这种方法也有其限制，如可能导致较慢的学习速率，因为更新的频率较低。此外，累积的误差可能不总是导致更好的值函数估计。

在实践中，选择是否使用批处理更新取决于特定的应用和资源限制。
---

%%end highlights%%

