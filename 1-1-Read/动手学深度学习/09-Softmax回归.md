## 从回归到分类

 [[Excalidraw/Drawing 2023-11-20 20.29.28.excalidraw.md#^PtUecmbp|回归和分类的区别]]

## 从回归到多类分类 - 均方误差
- 对类别进行一位的有效编码
- 是用均方损失训练 
- 最大值作为预测 $\hat y = argmaxO_j$

## 从回归到多类分类 - 无校验比例
- 对类别进行一位有效编码
- 最大值作为预测 $\hat y = argmaxO_j$
- 需要更置信的识别正确类（大余量）$o _ { y } - o _ { i } \geq A ( y , i )$，即对于预测中选定的那个输出的值远大于其余的输出的预测值

## 从回归到类分类 - 校验比例
- 输出匹配概率 （非负，和为1）
	- 从整体来看，将输出O作用到一个softmax函数上面，使得所有的y满足非负且和为1
	  $$\hat y = softmax(o)$$
	- 从细节看，就是对每一个输出的$O_i$，将其进行指数化，指数化之后，肯定大于0，再除以所有输出做指数，这样能实现所有y的和为1。
$$\hat y _ { i } = \frac { e x p ( 0 _ { i } ) } { \sum _ { k } e x p ( o _ { k } ) }$$
- 概率$y$ 和 $\hat y$ 作为损失，比较真实的概率和预测的概率之间的损失
## Softmax和交叉熵损失
- 交叉熵常用来衡量两个概率的区别$$H ( p , q ) = \sum _ { i } - p _ { i } \log ( q _ { i } )$$
	- $\mathbf{p}$ 是真实的标签概率分布。在实际应用中，这通常是一个“one-hot”向量，其中正确类别的位置为 1，其他位置为 0，形如$[0,1,0,0]$。
	- $\mathbf{q}$ 是模型预测的概率分布，由 softmax 函数输出。
	- $p_j$ 和 $q_j$ 分别是向量 $\mathbf{p}$ 和 $\mathbf{q}$ 在第 $j$ 个位置的元素。
	- $\log$ 是自然对数。
- 将交叉熵作为损失$$l ( y , \widehat { y } ) = - \sum _ { i } y _ { i } \log \widehat { y } _ { i } = - \log \widehat { y } _ { y } $$
	-  $\hat{y}_{y}$ 是模型预测的概率分布中，对应于真实类别 $y$ 的那个概率值。
	- $-\log(\hat{y}_{y})$ 计算的是对这个预测概率取负对数。
其中 $y_i$ 是真实标签中第 $i$ 个元素的值，$\hat y{_i}$ 是模型预测的概率分布中第 $i$ 个元素的值。
- 其梯度是真实概率和预测概率的区别$$\partial _ { o _ { i } } l ( y , y ) = \operatorname { s o f t m a x} ( 0 ) _ { i } - y _ { i }$$
#### 交叉熵例子：
举一个交叉熵的例子：假设一个三分类问题，其中一个样本的真实标签是第二个类别。那么，真实标签的概率分布 $\mathbf{p}$ 可以表示为一个 one-hot 向量 `[0, 1, 0]`。假设模型的预测概率分布 $\mathbf{q}$ 是 `[0.3, 0.6, 0.1]`。
交叉熵损失可以计算为：
$$H(\mathbf{p}, \mathbf{q}) = -(0 \cdot \log(0.3) + 1 \cdot \log(0.6) + 0 \cdot \log(0.1)) = -\log(0.6)$$
交叉熵损失的值越小，表示模型预测的概率分布与真实的概率分布越接近，即模型预测越准确。在训练过程中，目标是最小化交叉熵损失，从而使模型的预测更接近真实标签。

#### 交叉熵损失例子：
让我们通过一个简单的例子来说明如何计算**交叉熵损失**。假设我们有一个分类问题，目标是将输入数据分类到三个类别中的一个（类别0、类别1或类别2）。我们的模型将为每个类别提供一个预测概率。
假设对于一个特定的输入样本，模型的输出（softmax层之后的概率）和真实的类别如下：
- **模型输出（预测概率）**：假设模型预测的概率是 $[0.1, 0.2, 0.7]$。这意味着模型预测输入样本属于类别0的概率是10%，类别1的概率是20%，类别2的概率是70%。
- **真实标签**：假设真实的类别是类别1。在 one-hot 编码中，我们可以表示为 $[0, 1, 0]$。
交叉熵损失的计算公式是：
$$L = -\sum_{i=1}^{N} y_i \log(p_i)$$
其中，$y_i$ 是真实标签中第 $i$ 个元素的值，$p_i$ 是模型预测的概率分布中第 $i$ 个元素的值。
根据这个公式，我们可以计算损失：
$$L = -(0 \times \log(0.1) + 1 \times \log(0.2) + 0 \times \log(0.7))$$
$$= -(\log(0.2))$$
$$= -(-1.60943791243)$$
$$= 1.60943791243$$
所以，对于这个特定的样本，交叉熵损失大约是 1.61。这个值越低，表示模型的预测越接近真实标签。在训练过程中，模型的目标是最小化这个损失值，从而使预测更加准确。

## 总结
- Softmax回归是一个多类分类模型
- 是用Softmax操作子得到每个类的预测置信度
- 使用交叉熵来衡量预测和真实（label）的区别
- --------
# 几种常用的损失函数

## 均方损失 （l2 Loss）
$$l ( y , y ^ { \prime } ) = \frac { 1 } { 2 } ( y - y ^ { \prime } ) ^ { 2 }$$

## 绝对值损失函数 （L1 Loss）

$$l ( y , y ^ { \prime } ) = | y - y ^ { \prime } |$$
## Huber's Robust Loss
$$l ( y , y' ) = \{ \begin{array}  { l l  }  { | y - y' |  - \frac { 1 } { 2 } } & if{  | y - y ^ { \prime } | \gt 1 } \\ { \frac { 1 } { 2 } ( y - y ^ { \prime } ) ^ { 2 } } & { otherwise } \end{array}$$
如果$y'$和$y$的差距的绝对值小于1，使用绝对值损失，否则使用均方损失。

----
# 图片分类数据集
我们使用`Fashion-MNIST`数据集
