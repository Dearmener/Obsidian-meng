#机器学习笔记
#信息熵 [信息熵是什么？ - YJango的回答 - 知乎](https://www.zhihu.com/question/22178202/answer/577936758)

----
熵代表着人们对一个事情的熟悉程度，熵越高，对一个事物的了解就越低。我们用抛硬币作为熵的基本单位，听起来很离谱，接下来我们详细说明。
## 抛硬币
抛硬币是一个正反对立为 1/2 概率的事件。假如我们在挑选一个西瓜，我们有 1/8 的概率判断其为好瓜，这个概率，可以类比为抛了 3 次硬币。因此我们可以引出一个公式$2^{n} = m$，反向计算可得$n=log_{2}m$。n 代表着我们要抛几次硬币。m 是概率的倒数。

## 信息熵
$$Ent(X) = - \sum_{i=1}^{n} P(x_i) \cdot \log_{2}(P(x_i))$$
假如我们有一个如下的情况
![[Pasted image 20231228002157.png]]
我们看第一层的 1/6,1/3,1/2，这代表着三个不同的事件的发生概率，我们命名为 A，B，C。对于 B 事件，发生的概率为 1/3，我们可以拆分为两个 1/6，对于C事件，发生的概率为 1/2，我们可以拆分为三个 1/6，整个空间被拆分为 6 份。
经过上一步的拆分，进行计算：对于事件 C，其占整体的6 份中的3份，按照$n=log_{2}m$，我们计算为$log_{2}6$，取其中的3份$log_{2}3$，再相减$log_{2}6 - log_{2}3$，这由于事件 C 发生的概率为 1/2，所以事件 C 的信息熵为$\frac{1}{2} \{ log_{2}6-log_{2}3 \} = \frac{1}{2} log_{2}2$

>这个问题中的减法操作是为了计算事件 C 的信息量，而这个信息量是基于事件 C 在整个概率空间中所占的比例。在这个特定的例子中，我们将整个概率空间视为六个等概率（每个概率为 1/6）的部分。事件 C，其概率为 1/2，相当于在这六部分中占据了三部分（因为 1/2 = 3/6）。整个概率空间的信息量是 $\log_{2}6$，而事件 C 占据的三份中每一份的信息量是 $\log_{2}3$。
>
>减法操作 $\log_{2}6 - \log_{2}3$ 实际上是在找出单个部分（即事件 C 的一部分）相对于整个概率空间的信息量。这种减法是基于对数的性质，即 $\log_{2}6 - \log_{2}3 = \log_{2}\frac{6}{3} = \log_{2}2$。这反映了从整个概率空间的角度看，事件 C 占据的比例所对应的信息量。
>
>因此，减法操作是为了从整体的信息量中提取出特定事件（在这个例子中是事件 C）所对应的信息量的部分。通过这种方式，我们可以评估单个事件在整个概率分布中所占的“信息重量”。这种计算方法虽然不是直接应用标准的信息熵公式，但它提供了对事件在概率空间中所占比重的直观理解。

------
# GPT的回答
了解信息熵的定义如何得出确实需要对信息理论的基本概念有所了解。让我们通过一个简单的例子来阐述信息熵是如何定义和计算的。
### 基本概念
信息熵（Entropy），最初由克劳德·香农在信息理论中引入，用于衡量信息的不确定性或随机性。信息熵的基本思想是：一个事件的信息量与其发生的概率成反比。即，事件发生的概率越低，它携带的信息量就越大。
### 信息量的定义
信息量的定义基于这样一个观点：如果我们有一个概率为 $P$ 的事件，那么这个事件的信息量 $I$ 可以定义为：
$$I = \log_{2}(\frac{1}{P})$$
这里使用对数的原因是要保证当两个独立事件同时发生时，它们的总信息量等于各自信息量的和。对数的底数为2是因为信息通常以二进制（比如比特）来表示。
### 信息熵的定义
信息熵（Entropy），表示为 $Ent(X)$，则是这些不同信息量的期望值，即平均信息量。它被定义为：
$$Ent(X) = - \sum_{i=1}^{n} P(x_i) \cdot \log_{2}(P(x_i))$$
其中，$P(x_i)$ 是事件 $x_i$ 发生的概率。负号确保了信息熵是正值。
### 例子：投掷硬币
考虑一个简单的例子：投掷一枚公正的硬币。硬币落地时正面朝上和反面朝上的概率都是 1/2。因此，每种结果的信息量是：
$$I = \log_{2}(2) = 1 \text{ 比特}$$
这意味着每次投掷都会产生1比特的信息量。因为这两种结果（正面和反面）是硬币投掷的全部可能结果，所以信息熵为：
$$Ent(X) = - \left( \frac{1}{2} \log_{2}(\frac{1}{2}) + \frac{1}{2} \log_{2}(\frac{1}{2}) \right) = 1 \text{ 比特}$$
这表明，在投掷公正硬币的情况下，我们每次获得的平均信息量是1比特。
### 结论
信息熵提供了一种量化不同概率事件产生的信息量的方法。这对于理解和分析任何涉及不确定性和随机性的系统都非常有用，例如在数据传输、密码学和数据压缩等领域。